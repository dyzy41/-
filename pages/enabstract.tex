This dissertation focuses on the key challenges in the field of High-Resolution Remote Sensing Image Change Detection (RSCD) and conducts a systematic study revolving around four core aspects: innovation in the fundamental paradigm of change detection, enhancement of feature extraction, bi-temporal feature interaction, and suppression of pseudo-changes. It proposes a series of systematic solutions aimed at comprehensively enhancing the accuracy, robustness, and generalization ability of change detection models, thereby providing advanced technical support for applications such as environmental monitoring, urban planning, and disaster assessment.

First, in terms of innovating the fundamental paradigm of change detection, this dissertation proposes a groundbreaking Siamese-Encoder-Exchange-Decoder (SEED) framework to overcome the limitations of traditional architectures. This paradigm abandons the explicit "difference calculation" or "feature fusion" modules commonly relied upon by existing models. Instead, solely through feature exchange within a Siamese network, it drives the model to implicitly learn and locate the "inconsistency" of bi-temporal features based on a "pixel consistency principle." The SEED architecture is not only structurally simpler and more parameter-efficient, avoiding potential information loss from traditional fusion operations, but also theoretically unifies the frameworks of change detection and semantic segmentation. Experimental results demonstrate that this paradigm possesses excellent performance and high flexibility, offering a new perspective for the design and technological migration of future change detection models.

Second, to enhance feature extraction, this work addresses the limited generalization ability of traditional models caused by insufficient semantic priors by exploring two technical paths based on AI foundation models. The first is the proposal of the ChangeCLIP model, which introduces a multimodal vision-language pre-trained model (CLIP) to change detection for the first time. This model leverages CLIP's unsupervised inference capabilities to generate text prompts and designs a Differential Feature Compensation (DFC) module along with a vision-language-driven decoder based on low-rank bilinear attention to explicitly inject rich semantic information into remote sensing imagery, effectively compensating for the deficiencies of single-modality vision in understanding complex scenes. The second is the PeftCD framework, which systematically investigates how to efficiently adapt powerful vision foundation models like SAM2 and DINOv3 to remote sensing scenarios using Parameter-Efficient Fine-Tuning (PEFT) techniques (e.g., LoRA, Adapter). While preserving the strong general priors of the foundation models, this framework achieves rapid migration to downstream tasks at a very low training cost, demonstrating state-of-the-art performance across multiple change detection datasets.

Furthermore, regarding the representation of bi-temporal image differences, this dissertation designs several innovative modules for bi-temporal feature processing. First, EfficientCD is proposed based on the Euclidean distance metric; it explicitly integrates the geometric distance of bi-temporal features into a layer-wise decoding process, intuitively enhancing sensitivity to change regions. Concurrently, a Feature-Exchange Pyramid module is introduced by combining feature exchange strategies to strengthen the information flow between bi-temporal images. Second, the Layer-Exchange Network (LENet) is proposed, which incorporates channel-space dual differences. It extends change feature learning from the spatial dimension to the channel dimension by constructing a channel-space difference learning module, prompting the model to represent changes from multiple dimensions. These methods significantly improve the model's ability to perceive and locate complex change features.

Finally, for the suppression of pseudo-changes, this dissertation proposes a solution based on content and style disentanglement to address the domain shift problem caused by "style" differences in bi-temporal images arising from factors like illumination and seasonal changes. The designed Content-Style Disentanglement Network (CSDNet) actively separates image information at the feature level into "content" information (representing the essential properties of ground objects) and "style" information (representing imaging conditions) through a Content-Style Disentanglement Module and a Contextual Content Refinement Module. By stripping away style information via instance normalization and adaptively filtering out irrelevant style noise with a gating mechanism, the model can focus more on genuine changes in ground object content, thereby significantly improving its robustness under complex imaging conditions.

In summary, this dissertation conducts research on change detection through four aspects: innovation in fundamental paradigms, enhancement of feature extraction, difference representation, and pseudo-change suppression, injecting new ideas, methods, and paradigms into the development of high-resolution remote sensing image change detection technology. Exhaustive experiments on multiple public datasets demonstrate that the series of models proposed in this work have achieved leading or highly competitive performance, possessing significant theoretical value and practical engineering application prospects.
